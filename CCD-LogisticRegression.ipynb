{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea4f1a5d",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88726778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc72c4dd",
   "metadata": {},
   "source": [
    "We started with the Logistic Regression algorithm for classifying the data. We used the optimizers class provided in A4 file. For the custom built classifier class we modified the code we have used in A4 to perform binary classification. We have modified error and gradient functions to give logarithmic ascent. We have replaced the softmax function with sigmoid activation function for the output layer to fit the model better for the binary classification. The output of the layer i is now being fed to the sigmoid activation function. The sigmoid function is an S-shaped curve that maps any real-valued number to a value between 0 and 1. It is defined as:\n",
    "\n",
    "sigmoid(x) = 1 / (1 + exp(-x)).\n",
    "\n",
    "After applying the sigmoid function we get the predicted probabilities of legit or fraudulent transactions. \n",
    "\n",
    "\n",
    "We then use argmax function to decide which class the particular sample belongs to. Since the dataset is imbalanced we thought it might be helpful to use K-cross validation to get a better assessment by evaluating the model on multiple train-test splits, considering different combinations of minority and majority class samples. We used code form A3 for function generate_k_fold_cross_validation_sets. We have modified the function run_k_fold_cross_validation from the same assignment to include percentage of correct classification values in the result data frame. We have used different network architectures and compared the results given by each of them. The dataset is quite imbalanced with only 492 fradulent transactions in the total of 284807 samples. So, we decided to use the SMOTE(Synthetic Minority Over-sampling Technique) to introduce synthetic samples of the minority class in hope that it will help train the model better for discriminating against the legit and fraudulent transaction. SMOTE determines K-nearest neighbours of each minority instance and generates synthetic samples for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5e58ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting optimizers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile optimizers.py\n",
    "import numpy as np\n",
    "\n",
    "######################################################################\n",
    "## class Optimizers()\n",
    "######################################################################\n",
    "\n",
    "class Optimizers():\n",
    "\n",
    "    def __init__(self, all_weights):\n",
    "        '''all_weights is a vector of all of a neural networks weights concatenated into a one-dimensional vector'''\n",
    "        \n",
    "        self.all_weights = all_weights\n",
    "\n",
    "        # The following initializations are only used by adam.\n",
    "        # Only initializing m, v, beta1t and beta2t here allows multiple calls to adam to handle training\n",
    "        # with multiple subsets (batches) of training data.\n",
    "        self.mt = np.zeros_like(all_weights)\n",
    "        self.vt = np.zeros_like(all_weights)\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.999\n",
    "        self.beta1t = 1\n",
    "        self.beta2t = 1\n",
    "\n",
    "        \n",
    "    def sgd(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
    "        '''\n",
    "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "            with respect to each weight.\n",
    "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            \n",
    "            grad = gradient_f(*fargs)\n",
    "            \n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= learning_rate * grad\n",
    "\n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'sgd: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n",
    "    def adam(self, error_f, gradient_f, fargs=[], n_epochs=100, learning_rate=0.001, verbose=True, error_convert_f=None):\n",
    "        '''\n",
    "error_f: function that requires X and T as arguments (given in fargs) and returns mean squared error.\n",
    "gradient_f: function that requires X and T as arguments (in fargs) and returns gradient of mean squared error\n",
    "            with respect to each weight.\n",
    "error_convert_f: function that converts the standardized error from error_f to original T units.\n",
    "        '''\n",
    "\n",
    "        alpha = learning_rate  # learning rate called alpha in original paper on adam\n",
    "        epsilon = 1e-8\n",
    "        error_trace = []\n",
    "        epochs_per_print = n_epochs // 10\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            error = error_f(*fargs)\n",
    "            \n",
    "            grad = gradient_f(*fargs)\n",
    "            \n",
    "\n",
    "\n",
    "            self.mt[:] = self.beta1 * self.mt + (1 - self.beta1) * grad\n",
    "            self.vt[:] = self.beta2 * self.vt + (1 - self.beta2) * grad * grad\n",
    "            self.beta1t *= self.beta1\n",
    "            self.beta2t *= self.beta2\n",
    "\n",
    "            m_hat = self.mt / (1 - self.beta1t)\n",
    "            v_hat = self.vt / (1 - self.beta2t)\n",
    "\n",
    "            # Update all weights using -= to modify their values in-place.\n",
    "            self.all_weights -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "    \n",
    "            if error_convert_f:\n",
    "                error = error_convert_f(error)\n",
    "            error_trace.append(error)\n",
    "\n",
    "            if verbose and ((epoch + 1) % max(1, epochs_per_print) == 0):\n",
    "                print(f'Adam: Epoch {epoch+1:d} Error={error:.5f}')\n",
    "\n",
    "        return error_trace\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.ion()\n",
    "\n",
    "    def parabola(wmin):\n",
    "        return ((w - wmin) ** 2)[0]\n",
    "\n",
    "    def parabola_gradient(wmin):\n",
    "        return 2 * (w - wmin)\n",
    "\n",
    "    w = np.array([0.0])\n",
    "    optimizer = Optimizers(w)\n",
    "\n",
    "    wmin = 5\n",
    "    optimizer.sgd(parabola, parabola_gradient, [wmin],\n",
    "                  n_epochs=500, learning_rate=0.1)\n",
    "\n",
    "    print(f'sgd: Minimum of parabola is at {wmin}. Value found is {w}')\n",
    "\n",
    "    w = np.array([0.0])\n",
    "    optimizer = Optimizers(w)\n",
    "    optimizer.adam(parabola, parabola_gradient, [wmin],\n",
    "                   n_epochs=500, learning_rate=0.1)\n",
    "    \n",
    "    print(f'adam: Minimum of parabola is at {wmin}. Value found is {w}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ad868e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optimizers\n",
    "import sys  # for sys.float_info.epsilon\n",
    "\n",
    "######################################################################\n",
    "## class NeuralNetwork()\n",
    "######################################################################\n",
    "\n",
    "class NeuralNetwork():\n",
    "\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
    "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
    "            self.n_hiddens_per_layer = []\n",
    "        else:\n",
    "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
    "\n",
    "        # Initialize weights, by first building list of all weight matrix shapes.\n",
    "        n_in = n_inputs\n",
    "        shapes = []\n",
    "        for nh in self.n_hiddens_per_layer:\n",
    "            shapes.append((n_in + 1, nh))\n",
    "            n_in = nh\n",
    "        shapes.append((n_in + 1, n_outputs))\n",
    "\n",
    "        # self.all_weights:  vector of all weights\n",
    "        # self.Ws: list of weight matrices by layer\n",
    "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
    "\n",
    "        # Define arrays to hold gradient values.\n",
    "        # One array for each W array with same shape.\n",
    "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
    "\n",
    "        self.trained = False\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "        self.Tmeans = None\n",
    "        self.Tstds = None\n",
    "\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        # vector of all weights built by horizontally stacking flatenned matrices\n",
    "        # for each layer initialized with uniformly-distributed values.\n",
    "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
    "                                 for shape in shapes])\n",
    "        # Build list of views by reshaping corresponding elements from vector of all weights\n",
    "        # into correct shape for each layer.\n",
    "        views = []\n",
    "        start = 0\n",
    "        for shape in shapes:\n",
    "            size =shape[0] * shape[1]\n",
    "            views.append(all_weights[start:start + size].reshape(shape))\n",
    "            start += size\n",
    "        return all_weights, views\n",
    "\n",
    "\n",
    "    # Return string that shows how the constructor was called\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
    "\n",
    "\n",
    "    # Return string that is more informative to the user about the state of this neural network.\n",
    "    def __str__(self):\n",
    "        result = self.__repr__()\n",
    "        if len(self.error_trace) > 0:\n",
    "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
    "\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "       \n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "            self.Tmeans = T.mean(axis=0)\n",
    "            self.Tstds = T.std(axis=0)\n",
    "            \n",
    "        # Standardize X and T\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        T = (T - self.Tmeans) / self.Tstds\n",
    "\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert value from error_f into error in original T units, \n",
    "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
    "        # multiple outputs does not correctly unstandardize the error.\n",
    "        if len(self.Tstds) == 1:\n",
    "            error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
    "        else:\n",
    "            error_convert_f = lambda err: np.sqrt(err)[0] # to scalar\n",
    "            \n",
    "\n",
    "        if method == 'sgd':\n",
    "\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f,\n",
    "                                        fargs=[X, T], n_epochs=n_epochs,\n",
    "                                        learning_rate=learning_rate,\n",
    "                                        verbose=True,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "\n",
    "        elif method == 'adam':\n",
    "\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f,\n",
    "                                         fargs=[X, T], n_epochs=n_epochs,\n",
    "                                         learning_rate=learning_rate,\n",
    "                                         verbose=True,\n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "        return self\n",
    "\n",
    "    def relu(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s\n",
    "\n",
    "    def grad_relu(self, s):\n",
    "        return (s > 0).astype(int)\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        '''X assumed already standardized. Output returned as standardized.'''\n",
    "        self.Ys = [X]\n",
    "        for W in self.Ws[:-1]:\n",
    "            if self.activation_function == 'relu':\n",
    "                self.Ys.append(self.relu(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "            else:\n",
    "                self.Ys.append(np.tanh(self.Ys[-1] @ W[1:, :] + W[0:1, :]))\n",
    "        last_W = self.Ws[-1]\n",
    "        self.Ys.append(self.Ys[-1] @ last_W[1:, :] + last_W[0:1, :])\n",
    "        return self.Ys\n",
    "\n",
    "    # Function to be minimized by optimizer method, mean squared error\n",
    "    def error_f(self, X, T):\n",
    "        Ys = self.forward_pass(X)\n",
    "        mean_sq_error = np.mean((T - Ys[-1]) ** 2)\n",
    "        return mean_sq_error\n",
    "\n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = T - self.Ys[-1]\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "            # gradient of all but bias weights\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta\n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0)\n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self, X):\n",
    "        '''X assumed to not be standardized'''\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        Ys = self.forward_pass(X)\n",
    "        Y = Ys[-1]\n",
    "        # Unstandardize output Y before returning it\n",
    "        return Y * self.Tstds + self.Tmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "136925d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkClassifier(NeuralNetwork):\n",
    "\n",
    "\n",
    "    def __init__(self, n_inputs, n_hiddens_per_layer, n_outputs, activation_function='tanh'):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        # Set self.n_hiddens_per_layer to [] if argument is 0, [], or [0]\n",
    "        if n_hiddens_per_layer == 0 or n_hiddens_per_layer == [] or n_hiddens_per_layer == [0]:\n",
    "            self.n_hiddens_per_layer = []\n",
    "        else:\n",
    "            self.n_hiddens_per_layer = n_hiddens_per_layer\n",
    "\n",
    "        # Initialize weights, by first building list of all weight matrix shapes.\n",
    "        n_in = n_inputs\n",
    "        shapes = []\n",
    "        for nh in self.n_hiddens_per_layer:\n",
    "            shapes.append((n_in + 1, nh))\n",
    "            n_in = nh\n",
    "        shapes.append((n_in + 1, n_outputs))\n",
    "\n",
    "        # self.all_weights:  vector of all weights\n",
    "        # self.Ws: list of weight matrices by layer\n",
    "        self.all_weights, self.Ws = self.make_weights_and_views(shapes)\n",
    "\n",
    "        # Define arrays to hold gradient values.\n",
    "        # One array for each W array with same shape.\n",
    "        self.all_gradients, self.dE_dWs = self.make_weights_and_views(shapes)\n",
    "\n",
    "        self.trained = False\n",
    "        self.total_epochs = 0\n",
    "        self.error_trace = []\n",
    "        self.Xmeans = None\n",
    "        self.Xstds = None\n",
    "        #self.Tmeans = None\n",
    "        #self.Tstds = None\n",
    "\n",
    "\n",
    "    def make_weights_and_views(self, shapes):\n",
    "        # vector of all weights built by horizontally stacking flatenned matrices\n",
    "        # for each layer initialized with uniformly-distributed values.\n",
    "        all_weights = np.hstack([np.random.uniform(size=shape).flat / np.sqrt(shape[0])\n",
    "                                 for shape in shapes])\n",
    "        # Build list of views by reshaping corresponding elements from vector of all weights\n",
    "        # into correct shape for each layer.\n",
    "        views = []\n",
    "        start = 0\n",
    "        for shape in shapes:\n",
    "            size = shape[0] * shape[1]\n",
    "            views.append(all_weights[start:start + size].reshape(shape))\n",
    "            start += size\n",
    "        return all_weights, views\n",
    "\n",
    "\n",
    "    # Return string that shows how the constructor was called\n",
    "    def __repr__(self):\n",
    "        return f'{type(self).__name__}({self.n_inputs}, {self.n_hiddens_per_layer}, {self.n_outputs}, \\'{self.activation_function}\\')'\n",
    "\n",
    "\n",
    "    # Return string that is more informative to the user about the state of this neural network.\n",
    "    def __str__(self):\n",
    "        result = self.__repr__()\n",
    "        if len(self.error_trace) > 0:\n",
    "            return self.__repr__() + f' trained for {len(self.error_trace)} epochs, final training error {self.error_trace[-1]:.4f}'\n",
    "\n",
    "\n",
    "    def train(self, X, T, n_epochs, learning_rate, method='sgd', verbose=True):\n",
    "        \n",
    "        #n_classes = len(np.unique(T))\n",
    "        # Convert y to one-hot encoding\n",
    "        #T_onehot = np.eye(n_classes)\n",
    "        \n",
    "        # Setup standardization parameters\n",
    "        if self.Xmeans is None:\n",
    "            self.Xmeans = X.mean(axis=0)\n",
    "            self.Xstds = X.std(axis=0)\n",
    "            self.Xstds[self.Xstds == 0] = 1  # So we don't divide by zero when standardizing\n",
    "#             self.Tmeans = T.mean(axis=0)\n",
    "#             self.Tstds = T.std(axis=0)\n",
    "            self.T = T\n",
    "            self.classes = np.unique(T).reshape(-1,1)\n",
    "            #X = X.reshape(-1,1)\n",
    "        Xin = X\n",
    "        # Standardize X\n",
    "        X = (X - self.Xmeans) / self.Xstds\n",
    "        #T = (T - self.Tmeans) / self.Tstds\n",
    "        T_onehot = self.makeIndicatorVars(T)\n",
    "        # Instantiate Optimizers object by giving it vector of all weights\n",
    "        optimizer = optimizers.Optimizers(self.all_weights)\n",
    "\n",
    "        # Define function to convert value from error_f into error in original T units, \n",
    "        # but only if the network has a single output. Multiplying by self.Tstds for \n",
    "        # multiple outputs does not correctly unstandardize the error.\n",
    "        #if len(self.Tstds) == 1:\n",
    "            #error_convert_f = lambda err: (np.sqrt(err) * self.Tstds)[0] # to scalar\n",
    "        #else:\n",
    "#         error_convert_f = lambda err: np.mean(((self.use(Xin)[0]) != (T)))\n",
    "        error_convert_f = lambda err: np.exp(-err)\n",
    "            \n",
    "\n",
    "        if method == 'sgd':\n",
    "            error_trace = optimizer.sgd(self.error_f, self.gradient_f, fargs=[X, T_onehot], \n",
    "                                        n_epochs=n_epochs, learning_rate=learning_rate, \n",
    "                                        verbose=verbose,\n",
    "                                        error_convert_f=error_convert_f)\n",
    "        elif method == 'adam':\n",
    "            error_trace = optimizer.adam(self.error_f, self.gradient_f, fargs=[X, T_onehot], \n",
    "                                         n_epochs=n_epochs, learning_rate=learning_rate, \n",
    "                                         verbose=verbose, \n",
    "                                         error_convert_f=error_convert_f)\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"method must be 'sgd' or 'adam'\")\n",
    "        \n",
    "        self.error_trace = error_trace\n",
    "\n",
    "        # Return neural network object to allow applying other methods after training.\n",
    "        #  Example:    Y = nnet.train(X, T, 100, 0.01).use(X)\n",
    "        return self\n",
    "\n",
    "    def relu(self, s):\n",
    "        s[s < 0] = 0\n",
    "        return s\n",
    "\n",
    "    def grad_relu(self, s):\n",
    "        return (s > 0).astype(int)\n",
    "    \n",
    "\n",
    "        def error_f(self, X, T):\n",
    "        Ys = self.forward_pass(X)\n",
    "        log_probs = np.log(self.sigmoid(Ys[-1]))\n",
    "        cross_entropy = -np.mean(T * log_probs)#/T.shape[1]\n",
    "        #likelihood = np.exp((cross_entropy)/X.shape[0])\n",
    "        return cross_entropy\n",
    "    \n",
    "    # Gradient of function to be minimized for use by optimizer method\n",
    "    def gradient_f(self, X, T):\n",
    "        '''Assumes forward_pass just called with layer outputs in self.Ys.'''\n",
    "        error = T - self.sigmoid(self.Ys[-1])\n",
    "        n_samples = X.shape[0]\n",
    "        n_outputs = T.shape[1]\n",
    "        delta = - error / (n_samples * n_outputs)\n",
    "        n_layers = len(self.n_hiddens_per_layer) + 1\n",
    "        # Step backwards through the layers to back-propagate the error (delta)\n",
    "        for layeri in range(n_layers - 1, -1, -1):\n",
    "\n",
    "            self.dE_dWs[layeri][1:, :] = self.Ys[layeri].T @ delta \n",
    "            # gradient of just the bias weights\n",
    "            self.dE_dWs[layeri][0:1, :] = np.sum(delta, 0) \n",
    "            # Back-propagate this layer's delta to previous layer\n",
    "            if self.activation_function == 'relu':\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * self.grad_relu(self.Ys[layeri])\n",
    "            else:\n",
    "                delta = delta @ self.Ws[layeri][1:, :].T * (1 - self.Ys[layeri] ** 2)\n",
    "  \n",
    "        return self.all_gradients\n",
    "\n",
    "    def use(self, X):\n",
    "        #X assumed to not be standardized\n",
    "        \n",
    "        # Standardize X\n",
    "        X1 = (X - self.Xmeans) / self.Xstds\n",
    "        Ys = self.forward_pass(X1)\n",
    "        Y_probs = self.sigmoid(Ys[-1])#, axis = 1)\n",
    "        Y_classes = self.classes[np.argmax(Y_probs, axis=1)].reshape(-1, 1)\n",
    "        return Y_classes, Y_probs#, T2\n",
    "    \n",
    "\n",
    "    def sigmoid(self, s) :\n",
    "        return 1/(1 + np.exp(-s))\n",
    "                   \n",
    "\n",
    "    def grad_sigmoid(self, s): \n",
    "        return self.sigmoid(s) * (1 - self.sigmoid(s))\n",
    "    \n",
    "    def makeIndicatorVars(self, T):\n",
    "    # Make sure T is two-dimensional. Should be nSamples x 1.\n",
    "        if T.ndim == 1:\n",
    "            T = T.reshape((-1, 1))    \n",
    "        return (T == np.unique(T)).astype(int)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0609004a",
   "metadata": {},
   "source": [
    "### Data Import and Data Preparation\n",
    "We used the Kaggle Credit Card Fraud Detection Dataset : <a href=\"https://www.kaggle.com/mlg-ulb/creditcardfraud\">Link</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ab42127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data into a Dataframe\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "df_refine = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "421e5f77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b74fe88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values\n",
    "T = df.iloc[:, -1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc34ff11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.00000000e+00, -1.35980713e+00, -7.27811733e-02, ...,\n",
       "          1.33558377e-01, -2.10530535e-02,  1.49620000e+02],\n",
       "        [ 0.00000000e+00,  1.19185711e+00,  2.66150712e-01, ...,\n",
       "         -8.98309914e-03,  1.47241692e-02,  2.69000000e+00],\n",
       "        [ 1.00000000e+00, -1.35835406e+00, -1.34016307e+00, ...,\n",
       "         -5.53527940e-02, -5.97518406e-02,  3.78660000e+02],\n",
       "        ...,\n",
       "        [ 1.72788000e+05,  1.91956501e+00, -3.01253846e-01, ...,\n",
       "          4.45477214e-03, -2.65608286e-02,  6.78800000e+01],\n",
       "        [ 1.72788000e+05, -2.40440050e-01,  5.30482513e-01, ...,\n",
       "          1.08820735e-01,  1.04532821e-01,  1.00000000e+01],\n",
       "        [ 1.72792000e+05, -5.33412522e-01, -1.89733337e-01, ...,\n",
       "         -2.41530880e-03,  1.36489143e-02,  2.17000000e+02]]),\n",
       " array([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        ...,\n",
       "        [0],\n",
       "        [0],\n",
       "        [0]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2bbbd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [1]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = np.unique(T).reshape(-1,1)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ce8f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_k_fold_cross_validation_sets(X, T, n_folds, shuffle=True):\n",
    "\n",
    "    if shuffle:\n",
    "        # Randomly order X and T\n",
    "        randorder = np.arange(X.shape[0])\n",
    "        np.random.shuffle(randorder)\n",
    "        X = X[randorder, :]\n",
    "        T = T[randorder, :]\n",
    "\n",
    "    # Partition X and T into folds\n",
    "    n_samples = X.shape[0]\n",
    "    n_per_fold = round(n_samples / n_folds)\n",
    "    n_last_fold = n_samples - n_per_fold * (n_folds - 1)\n",
    "\n",
    "    folds = []\n",
    "    start = 0\n",
    "    for foldi in range(n_folds-1):\n",
    "        folds.append( (X[start:start + n_per_fold, :], T[start:start + n_per_fold, :]) )\n",
    "        start += n_per_fold\n",
    "    folds.append( (X[start:, :], T[start:, :]) )\n",
    "\n",
    "    # Yield k(k-1) assignments of Xtrain, Train, Xvalidate, Tvalidate, Xtest, Ttest\n",
    "\n",
    "    for validation_i in range(n_folds):\n",
    "        for test_i in range(n_folds):\n",
    "            if test_i == validation_i:\n",
    "                continue\n",
    "\n",
    "            train_i = np.setdiff1d(range(n_folds), [validation_i, test_i])\n",
    "\n",
    "            Xvalidate, Tvalidate = folds[validation_i]\n",
    "            Xtest, Ttest = folds[test_i]\n",
    "            if len(train_i) > 1:\n",
    "                Xtrain1 = np.vstack([folds[i][0] for i in train_i])\n",
    "                Ttrain1 = np.vstack([folds[i][1] for i in train_i])\n",
    "                sm = SMOTE(random_state=42)\n",
    "                Xtrain, Ttrain2 = sm.fit_resample(Xtrain1, Ttrain1)\n",
    "            else:\n",
    "                Xtrain1, Ttrain1 = folds[train_i[0]]\n",
    "                sm = SMOTE(random_state=42)\n",
    "                Xtrain, Ttrain2 = sm.fit_resample(Xtrain1, Ttrain1)\n",
    "            \n",
    "\n",
    "            yield Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359b37d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold_cross_validation(X, T, n_folds, list_of_n_hiddens, \n",
    "                                n_epochs, learning_rate, act_func, gpu = False):\n",
    "   \n",
    "    nn_arch = []\n",
    "    n_samples = X.shape[0]\n",
    "    n_features = X.shape[1]\n",
    "    n_outputs = T.shape[1]\n",
    "    Train = []\n",
    "    Validate = []\n",
    "    Test = []\n",
    "    results = []\n",
    "    if gpu:\n",
    "        print(\"Moving data and model to GPU. \\nCurrent CUDA device: %s\" %(torch.cuda.get_device_name(torch.cuda.current_device)))\n",
    "    else :\n",
    "        print(\"Running on CPU\")\n",
    "    for nh in list_of_n_hiddens: # Layer sizes\n",
    "        \n",
    "        \n",
    "        for Xtrain, Ttrain, Xvalidate, Tvalidate, Xtest, Ttest in generate_k_fold_cross_validation_sets(X, T, n_folds):\n",
    "                nnet =  NeuralNetworkClassifier(Xtrain.shape[1], nh, len(classes))\n",
    "#                 if gpu:\n",
    "#                     nnet = nnet.cuda()\n",
    "                print('Hidden Layers: ', nh)\n",
    "                start = time.time()\n",
    "                #nnet =  NeuralNetworkClassifier(Xtrain.shape[1], nh, len(classes))\n",
    "                nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)\n",
    "                Time = (time.time() - start) / 60/ 60\n",
    "       # append the  results of each experiment \n",
    "        #nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)\\\n",
    "                Y_trclasses, Y_trprobs = nnet.use(Xtrain)\n",
    "\n",
    "                Train.append(100 * np.mean(Y_trclasses == Ttrain))\n",
    "        \n",
    "        \n",
    "        #nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)\n",
    "                Y_vclasses, Y_vprobs = nnet.use(Xvalidate)\n",
    "\n",
    "                Validate.append(100 * np.mean(Y_vclasses == Tvalidate))\n",
    "        \n",
    "        #nnet.train(Xtrain, Ttrain, n_epochs, learning_rate, method='adam', verbose=True)\n",
    "                Y_tclasses, Y_tprobs = nnet.use(Xtest)\n",
    "\n",
    "                Test.append(100 * np.mean(Y_tclasses == Ttest))\n",
    "               \n",
    "        \n",
    "        #Averaging values over all the folds.\n",
    "        Train1 = sum(Train)/len(Train)\n",
    "        Validate1 = sum(Validate)/len(Validate)\n",
    "        Test1 = sum(Test)/len(Test)\n",
    "        results.append([nh, Train1, Validate1, Test1, Time])\n",
    "        # load these into a dataframe and give it some column titles\n",
    "    final_df = pd.DataFrame(results, columns=['Hidden Layers', 'Train', 'Validate', 'Test', 'Time'])\n",
    "    print(final_df)\n",
    "    return final_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905776d4",
   "metadata": {},
   "source": [
    "We have run experiments for three different architectures. One with a single hidden layer with 10 neurons. Another with 2 hidden layers with 50 neurons each. The final one has 3 hidden layers with 30 neurons each. We have run the experiments for 200 epochs for a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37fe7d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving data and model to GPU. \n",
      "Current CUDA device: NVIDIA TITAN V\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.89968\n",
      "Adam: Epoch 40 Error=0.98661\n",
      "Adam: Epoch 60 Error=0.99489\n",
      "Adam: Epoch 80 Error=0.99615\n",
      "Adam: Epoch 100 Error=0.99659\n",
      "Adam: Epoch 120 Error=0.99683\n",
      "Adam: Epoch 140 Error=0.99698\n",
      "Adam: Epoch 160 Error=0.99708\n",
      "Adam: Epoch 180 Error=0.99716\n",
      "Adam: Epoch 200 Error=0.99721\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88899\n",
      "Adam: Epoch 40 Error=0.98201\n",
      "Adam: Epoch 60 Error=0.99292\n",
      "Adam: Epoch 80 Error=0.99504\n",
      "Adam: Epoch 100 Error=0.99582\n",
      "Adam: Epoch 120 Error=0.99623\n",
      "Adam: Epoch 140 Error=0.99650\n",
      "Adam: Epoch 160 Error=0.99669\n",
      "Adam: Epoch 180 Error=0.99682\n",
      "Adam: Epoch 200 Error=0.99692\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.90962\n",
      "Adam: Epoch 40 Error=0.98517\n",
      "Adam: Epoch 60 Error=0.99382\n",
      "Adam: Epoch 80 Error=0.99560\n",
      "Adam: Epoch 100 Error=0.99647\n",
      "Adam: Epoch 120 Error=0.99699\n",
      "Adam: Epoch 140 Error=0.99727\n",
      "Adam: Epoch 160 Error=0.99745\n",
      "Adam: Epoch 180 Error=0.99758\n",
      "Adam: Epoch 200 Error=0.99767\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.86268\n",
      "Adam: Epoch 40 Error=0.97666\n",
      "Adam: Epoch 60 Error=0.99339\n",
      "Adam: Epoch 80 Error=0.99615\n",
      "Adam: Epoch 100 Error=0.99697\n",
      "Adam: Epoch 120 Error=0.99734\n",
      "Adam: Epoch 140 Error=0.99755\n",
      "Adam: Epoch 160 Error=0.99770\n",
      "Adam: Epoch 180 Error=0.99781\n",
      "Adam: Epoch 200 Error=0.99791\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88500\n",
      "Adam: Epoch 40 Error=0.98261\n",
      "Adam: Epoch 60 Error=0.99345\n",
      "Adam: Epoch 80 Error=0.99533\n",
      "Adam: Epoch 100 Error=0.99603\n",
      "Adam: Epoch 120 Error=0.99642\n",
      "Adam: Epoch 140 Error=0.99668\n",
      "Adam: Epoch 160 Error=0.99686\n",
      "Adam: Epoch 180 Error=0.99699\n",
      "Adam: Epoch 200 Error=0.99709\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.85541\n",
      "Adam: Epoch 40 Error=0.96892\n",
      "Adam: Epoch 60 Error=0.99220\n",
      "Adam: Epoch 80 Error=0.99554\n",
      "Adam: Epoch 100 Error=0.99647\n",
      "Adam: Epoch 120 Error=0.99700\n",
      "Adam: Epoch 140 Error=0.99730\n",
      "Adam: Epoch 160 Error=0.99751\n",
      "Adam: Epoch 180 Error=0.99766\n",
      "Adam: Epoch 200 Error=0.99779\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.87459\n",
      "Adam: Epoch 40 Error=0.98060\n",
      "Adam: Epoch 60 Error=0.99332\n",
      "Adam: Epoch 80 Error=0.99538\n",
      "Adam: Epoch 100 Error=0.99608\n",
      "Adam: Epoch 120 Error=0.99646\n",
      "Adam: Epoch 140 Error=0.99670\n",
      "Adam: Epoch 160 Error=0.99687\n",
      "Adam: Epoch 180 Error=0.99700\n",
      "Adam: Epoch 200 Error=0.99709\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.84444\n",
      "Adam: Epoch 40 Error=0.97276\n",
      "Adam: Epoch 60 Error=0.99236\n",
      "Adam: Epoch 80 Error=0.99536\n",
      "Adam: Epoch 100 Error=0.99630\n",
      "Adam: Epoch 120 Error=0.99677\n",
      "Adam: Epoch 140 Error=0.99707\n",
      "Adam: Epoch 160 Error=0.99727\n",
      "Adam: Epoch 180 Error=0.99742\n",
      "Adam: Epoch 200 Error=0.99753\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88732\n",
      "Adam: Epoch 40 Error=0.98489\n",
      "Adam: Epoch 60 Error=0.99525\n",
      "Adam: Epoch 80 Error=0.99660\n",
      "Adam: Epoch 100 Error=0.99701\n",
      "Adam: Epoch 120 Error=0.99723\n",
      "Adam: Epoch 140 Error=0.99737\n",
      "Adam: Epoch 160 Error=0.99748\n",
      "Adam: Epoch 180 Error=0.99756\n",
      "Adam: Epoch 200 Error=0.99762\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.87426\n",
      "Adam: Epoch 40 Error=0.98039\n",
      "Adam: Epoch 60 Error=0.99397\n",
      "Adam: Epoch 80 Error=0.99603\n",
      "Adam: Epoch 100 Error=0.99668\n",
      "Adam: Epoch 120 Error=0.99701\n",
      "Adam: Epoch 140 Error=0.99723\n",
      "Adam: Epoch 160 Error=0.99737\n",
      "Adam: Epoch 180 Error=0.99748\n",
      "Adam: Epoch 200 Error=0.99757\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88322\n",
      "Adam: Epoch 40 Error=0.98212\n",
      "Adam: Epoch 60 Error=0.99327\n",
      "Adam: Epoch 80 Error=0.99518\n",
      "Adam: Epoch 100 Error=0.99585\n",
      "Adam: Epoch 120 Error=0.99621\n",
      "Adam: Epoch 140 Error=0.99645\n",
      "Adam: Epoch 160 Error=0.99662\n",
      "Adam: Epoch 180 Error=0.99674\n",
      "Adam: Epoch 200 Error=0.99683\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88661\n",
      "Adam: Epoch 40 Error=0.98471\n",
      "Adam: Epoch 60 Error=0.99456\n",
      "Adam: Epoch 80 Error=0.99611\n",
      "Adam: Epoch 100 Error=0.99670\n",
      "Adam: Epoch 120 Error=0.99704\n",
      "Adam: Epoch 140 Error=0.99725\n",
      "Adam: Epoch 160 Error=0.99739\n",
      "Adam: Epoch 180 Error=0.99751\n",
      "Adam: Epoch 200 Error=0.99759\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88130\n",
      "Adam: Epoch 40 Error=0.98159\n",
      "Adam: Epoch 60 Error=0.99418\n",
      "Adam: Epoch 80 Error=0.99605\n",
      "Adam: Epoch 100 Error=0.99659\n",
      "Adam: Epoch 120 Error=0.99687\n",
      "Adam: Epoch 140 Error=0.99704\n",
      "Adam: Epoch 160 Error=0.99716\n",
      "Adam: Epoch 180 Error=0.99725\n",
      "Adam: Epoch 200 Error=0.99731\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.89000\n",
      "Adam: Epoch 40 Error=0.98443\n",
      "Adam: Epoch 60 Error=0.99469\n",
      "Adam: Epoch 80 Error=0.99627\n",
      "Adam: Epoch 100 Error=0.99682\n",
      "Adam: Epoch 120 Error=0.99711\n",
      "Adam: Epoch 140 Error=0.99731\n",
      "Adam: Epoch 160 Error=0.99745\n",
      "Adam: Epoch 180 Error=0.99755\n",
      "Adam: Epoch 200 Error=0.99763\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.86505\n",
      "Adam: Epoch 40 Error=0.97956\n",
      "Adam: Epoch 60 Error=0.99304\n",
      "Adam: Epoch 80 Error=0.99536\n",
      "Adam: Epoch 100 Error=0.99620\n",
      "Adam: Epoch 120 Error=0.99665\n",
      "Adam: Epoch 140 Error=0.99694\n",
      "Adam: Epoch 160 Error=0.99714\n",
      "Adam: Epoch 180 Error=0.99728\n",
      "Adam: Epoch 200 Error=0.99740\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88535\n",
      "Adam: Epoch 40 Error=0.98251\n",
      "Adam: Epoch 60 Error=0.99343\n",
      "Adam: Epoch 80 Error=0.99550\n",
      "Adam: Epoch 100 Error=0.99626\n",
      "Adam: Epoch 120 Error=0.99670\n",
      "Adam: Epoch 140 Error=0.99699\n",
      "Adam: Epoch 160 Error=0.99720\n",
      "Adam: Epoch 180 Error=0.99737\n",
      "Adam: Epoch 200 Error=0.99750\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.84191\n",
      "Adam: Epoch 40 Error=0.96651\n",
      "Adam: Epoch 60 Error=0.99192\n",
      "Adam: Epoch 80 Error=0.99532\n",
      "Adam: Epoch 100 Error=0.99629\n",
      "Adam: Epoch 120 Error=0.99678\n",
      "Adam: Epoch 140 Error=0.99720\n",
      "Adam: Epoch 160 Error=0.99745\n",
      "Adam: Epoch 180 Error=0.99762\n",
      "Adam: Epoch 200 Error=0.99775\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.89255\n",
      "Adam: Epoch 40 Error=0.98438\n",
      "Adam: Epoch 60 Error=0.99406\n",
      "Adam: Epoch 80 Error=0.99576\n",
      "Adam: Epoch 100 Error=0.99640\n",
      "Adam: Epoch 120 Error=0.99675\n",
      "Adam: Epoch 140 Error=0.99700\n",
      "Adam: Epoch 160 Error=0.99717\n",
      "Adam: Epoch 180 Error=0.99728\n",
      "Adam: Epoch 200 Error=0.99736\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.89050\n",
      "Adam: Epoch 40 Error=0.98567\n",
      "Adam: Epoch 60 Error=0.99461\n",
      "Adam: Epoch 80 Error=0.99614\n",
      "Adam: Epoch 100 Error=0.99673\n",
      "Adam: Epoch 120 Error=0.99705\n",
      "Adam: Epoch 140 Error=0.99727\n",
      "Adam: Epoch 160 Error=0.99742\n",
      "Adam: Epoch 180 Error=0.99753\n",
      "Adam: Epoch 200 Error=0.99762\n",
      "Hidden Layers:  [10]\n",
      "Adam: Epoch 20 Error=0.88294\n",
      "Adam: Epoch 40 Error=0.98240\n",
      "Adam: Epoch 60 Error=0.99395\n",
      "Adam: Epoch 80 Error=0.99577\n",
      "Adam: Epoch 100 Error=0.99639\n",
      "Adam: Epoch 120 Error=0.99674\n",
      "Adam: Epoch 140 Error=0.99697\n",
      "Adam: Epoch 160 Error=0.99713\n",
      "Adam: Epoch 180 Error=0.99724\n",
      "Adam: Epoch 200 Error=0.99733\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99526\n",
      "Adam: Epoch 40 Error=0.99745\n",
      "Adam: Epoch 60 Error=0.99755\n",
      "Adam: Epoch 80 Error=0.99766\n",
      "Adam: Epoch 100 Error=0.99775\n",
      "Adam: Epoch 120 Error=0.99783\n",
      "Adam: Epoch 140 Error=0.99790\n",
      "Adam: Epoch 160 Error=0.99797\n",
      "Adam: Epoch 180 Error=0.99804\n",
      "Adam: Epoch 200 Error=0.99812\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99675\n",
      "Adam: Epoch 40 Error=0.99708\n",
      "Adam: Epoch 60 Error=0.99740\n",
      "Adam: Epoch 80 Error=0.99806\n",
      "Adam: Epoch 100 Error=0.99851\n",
      "Adam: Epoch 120 Error=0.99861\n",
      "Adam: Epoch 140 Error=0.99869\n",
      "Adam: Epoch 160 Error=0.99877\n",
      "Adam: Epoch 180 Error=0.99887\n",
      "Adam: Epoch 200 Error=0.99894\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99702\n",
      "Adam: Epoch 40 Error=0.99711\n",
      "Adam: Epoch 60 Error=0.99728\n",
      "Adam: Epoch 80 Error=0.99745\n",
      "Adam: Epoch 100 Error=0.99760\n",
      "Adam: Epoch 120 Error=0.99774\n",
      "Adam: Epoch 140 Error=0.99786\n",
      "Adam: Epoch 160 Error=0.99803\n",
      "Adam: Epoch 180 Error=0.99819\n",
      "Adam: Epoch 200 Error=0.99831\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99655\n",
      "Adam: Epoch 40 Error=0.99722\n",
      "Adam: Epoch 60 Error=0.99737\n",
      "Adam: Epoch 80 Error=0.99760\n",
      "Adam: Epoch 100 Error=0.99805\n",
      "Adam: Epoch 120 Error=0.99842\n",
      "Adam: Epoch 140 Error=0.99854\n",
      "Adam: Epoch 160 Error=0.99863\n",
      "Adam: Epoch 180 Error=0.99870\n",
      "Adam: Epoch 200 Error=0.99879\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99676\n",
      "Adam: Epoch 40 Error=0.99713\n",
      "Adam: Epoch 60 Error=0.99733\n",
      "Adam: Epoch 80 Error=0.99756\n",
      "Adam: Epoch 100 Error=0.99783\n",
      "Adam: Epoch 120 Error=0.99853\n",
      "Adam: Epoch 140 Error=0.99873\n",
      "Adam: Epoch 160 Error=0.99880\n",
      "Adam: Epoch 180 Error=0.99886\n",
      "Adam: Epoch 200 Error=0.99892\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99720\n",
      "Adam: Epoch 40 Error=0.99759\n",
      "Adam: Epoch 60 Error=0.99770\n",
      "Adam: Epoch 80 Error=0.99786\n",
      "Adam: Epoch 100 Error=0.99803\n",
      "Adam: Epoch 120 Error=0.99823\n",
      "Adam: Epoch 140 Error=0.99858\n",
      "Adam: Epoch 160 Error=0.99883\n",
      "Adam: Epoch 180 Error=0.99889\n",
      "Adam: Epoch 200 Error=0.99894\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99688\n",
      "Adam: Epoch 40 Error=0.99709\n",
      "Adam: Epoch 60 Error=0.99726\n",
      "Adam: Epoch 80 Error=0.99763\n",
      "Adam: Epoch 100 Error=0.99786\n",
      "Adam: Epoch 120 Error=0.99818\n",
      "Adam: Epoch 140 Error=0.99851\n",
      "Adam: Epoch 160 Error=0.99873\n",
      "Adam: Epoch 180 Error=0.99879\n",
      "Adam: Epoch 200 Error=0.99885\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99672\n",
      "Adam: Epoch 40 Error=0.99703\n",
      "Adam: Epoch 60 Error=0.99757\n",
      "Adam: Epoch 80 Error=0.99841\n",
      "Adam: Epoch 100 Error=0.99859\n",
      "Adam: Epoch 120 Error=0.99867\n",
      "Adam: Epoch 140 Error=0.99873\n",
      "Adam: Epoch 160 Error=0.99878\n",
      "Adam: Epoch 180 Error=0.99882\n",
      "Adam: Epoch 200 Error=0.99887\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99642\n",
      "Adam: Epoch 40 Error=0.99733\n",
      "Adam: Epoch 60 Error=0.99744\n",
      "Adam: Epoch 80 Error=0.99754\n",
      "Adam: Epoch 100 Error=0.99764\n",
      "Adam: Epoch 120 Error=0.99772\n",
      "Adam: Epoch 140 Error=0.99780\n",
      "Adam: Epoch 160 Error=0.99789\n",
      "Adam: Epoch 180 Error=0.99804\n",
      "Adam: Epoch 200 Error=0.99856\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99505\n",
      "Adam: Epoch 40 Error=0.99761\n",
      "Adam: Epoch 60 Error=0.99775\n",
      "Adam: Epoch 80 Error=0.99790\n",
      "Adam: Epoch 100 Error=0.99806\n",
      "Adam: Epoch 120 Error=0.99859\n",
      "Adam: Epoch 140 Error=0.99874\n",
      "Adam: Epoch 160 Error=0.99883\n",
      "Adam: Epoch 180 Error=0.99888\n",
      "Adam: Epoch 200 Error=0.99892\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99568\n",
      "Adam: Epoch 40 Error=0.99766\n",
      "Adam: Epoch 60 Error=0.99773\n",
      "Adam: Epoch 80 Error=0.99786\n",
      "Adam: Epoch 100 Error=0.99797\n",
      "Adam: Epoch 120 Error=0.99805\n",
      "Adam: Epoch 140 Error=0.99811\n",
      "Adam: Epoch 160 Error=0.99819\n",
      "Adam: Epoch 180 Error=0.99826\n",
      "Adam: Epoch 200 Error=0.99841\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99705\n",
      "Adam: Epoch 40 Error=0.99694\n",
      "Adam: Epoch 60 Error=0.99718\n",
      "Adam: Epoch 80 Error=0.99739\n",
      "Adam: Epoch 100 Error=0.99765\n",
      "Adam: Epoch 120 Error=0.99788\n",
      "Adam: Epoch 140 Error=0.99822\n",
      "Adam: Epoch 160 Error=0.99856\n",
      "Adam: Epoch 180 Error=0.99868\n",
      "Adam: Epoch 200 Error=0.99875\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99670\n",
      "Adam: Epoch 40 Error=0.99696\n",
      "Adam: Epoch 60 Error=0.99730\n",
      "Adam: Epoch 80 Error=0.99761\n",
      "Adam: Epoch 100 Error=0.99795\n",
      "Adam: Epoch 120 Error=0.99828\n",
      "Adam: Epoch 140 Error=0.99842\n",
      "Adam: Epoch 160 Error=0.99853\n",
      "Adam: Epoch 180 Error=0.99864\n",
      "Adam: Epoch 200 Error=0.99873\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99692\n",
      "Adam: Epoch 40 Error=0.99727\n",
      "Adam: Epoch 60 Error=0.99734\n",
      "Adam: Epoch 80 Error=0.99747\n",
      "Adam: Epoch 100 Error=0.99759\n",
      "Adam: Epoch 120 Error=0.99770\n",
      "Adam: Epoch 140 Error=0.99782\n",
      "Adam: Epoch 160 Error=0.99807\n",
      "Adam: Epoch 180 Error=0.99864\n",
      "Adam: Epoch 200 Error=0.99873\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99722\n",
      "Adam: Epoch 40 Error=0.99759\n",
      "Adam: Epoch 60 Error=0.99827\n",
      "Adam: Epoch 80 Error=0.99844\n",
      "Adam: Epoch 100 Error=0.99852\n",
      "Adam: Epoch 120 Error=0.99859\n",
      "Adam: Epoch 140 Error=0.99865\n",
      "Adam: Epoch 160 Error=0.99873\n",
      "Adam: Epoch 180 Error=0.99881\n",
      "Adam: Epoch 200 Error=0.99890\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99636\n",
      "Adam: Epoch 40 Error=0.99717\n",
      "Adam: Epoch 60 Error=0.99731\n",
      "Adam: Epoch 80 Error=0.99747\n",
      "Adam: Epoch 100 Error=0.99760\n",
      "Adam: Epoch 120 Error=0.99779\n",
      "Adam: Epoch 140 Error=0.99843\n",
      "Adam: Epoch 160 Error=0.99854\n",
      "Adam: Epoch 180 Error=0.99862\n",
      "Adam: Epoch 200 Error=0.99870\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99644\n",
      "Adam: Epoch 40 Error=0.99717\n",
      "Adam: Epoch 60 Error=0.99730\n",
      "Adam: Epoch 80 Error=0.99747\n",
      "Adam: Epoch 100 Error=0.99766\n",
      "Adam: Epoch 120 Error=0.99795\n",
      "Adam: Epoch 140 Error=0.99852\n",
      "Adam: Epoch 160 Error=0.99865\n",
      "Adam: Epoch 180 Error=0.99873\n",
      "Adam: Epoch 200 Error=0.99881\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99666\n",
      "Adam: Epoch 40 Error=0.99742\n",
      "Adam: Epoch 60 Error=0.99769\n",
      "Adam: Epoch 80 Error=0.99786\n",
      "Adam: Epoch 100 Error=0.99806\n",
      "Adam: Epoch 120 Error=0.99855\n",
      "Adam: Epoch 140 Error=0.99870\n",
      "Adam: Epoch 160 Error=0.99875\n",
      "Adam: Epoch 180 Error=0.99880\n",
      "Adam: Epoch 200 Error=0.99885\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99651\n",
      "Adam: Epoch 40 Error=0.99724\n",
      "Adam: Epoch 60 Error=0.99731\n",
      "Adam: Epoch 80 Error=0.99748\n",
      "Adam: Epoch 100 Error=0.99784\n",
      "Adam: Epoch 120 Error=0.99842\n",
      "Adam: Epoch 140 Error=0.99851\n",
      "Adam: Epoch 160 Error=0.99858\n",
      "Adam: Epoch 180 Error=0.99864\n",
      "Adam: Epoch 200 Error=0.99868\n",
      "Hidden Layers:  [50, 50]\n",
      "Adam: Epoch 20 Error=0.99721\n",
      "Adam: Epoch 40 Error=0.99710\n",
      "Adam: Epoch 60 Error=0.99722\n",
      "Adam: Epoch 80 Error=0.99741\n",
      "Adam: Epoch 100 Error=0.99764\n",
      "Adam: Epoch 120 Error=0.99784\n",
      "Adam: Epoch 140 Error=0.99804\n",
      "Adam: Epoch 160 Error=0.99827\n",
      "Adam: Epoch 180 Error=0.99846\n",
      "Adam: Epoch 200 Error=0.99858\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99580\n",
      "Adam: Epoch 40 Error=0.99825\n",
      "Adam: Epoch 60 Error=0.99851\n",
      "Adam: Epoch 80 Error=0.99864\n",
      "Adam: Epoch 100 Error=0.99873\n",
      "Adam: Epoch 120 Error=0.99879\n",
      "Adam: Epoch 140 Error=0.99883\n",
      "Adam: Epoch 160 Error=0.99888\n",
      "Adam: Epoch 180 Error=0.99892\n",
      "Adam: Epoch 200 Error=0.99895\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99032\n",
      "Adam: Epoch 40 Error=0.99754\n",
      "Adam: Epoch 60 Error=0.99822\n",
      "Adam: Epoch 80 Error=0.99845\n",
      "Adam: Epoch 100 Error=0.99857\n",
      "Adam: Epoch 120 Error=0.99867\n",
      "Adam: Epoch 140 Error=0.99873\n",
      "Adam: Epoch 160 Error=0.99878\n",
      "Adam: Epoch 180 Error=0.99881\n",
      "Adam: Epoch 200 Error=0.99885\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99784\n",
      "Adam: Epoch 40 Error=0.99832\n",
      "Adam: Epoch 60 Error=0.99857\n",
      "Adam: Epoch 80 Error=0.99871\n",
      "Adam: Epoch 100 Error=0.99880\n",
      "Adam: Epoch 120 Error=0.99888\n",
      "Adam: Epoch 140 Error=0.99895\n",
      "Adam: Epoch 160 Error=0.99901\n",
      "Adam: Epoch 180 Error=0.99909\n",
      "Adam: Epoch 200 Error=0.99914\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99666\n",
      "Adam: Epoch 40 Error=0.99827\n",
      "Adam: Epoch 60 Error=0.99852\n",
      "Adam: Epoch 80 Error=0.99864\n",
      "Adam: Epoch 100 Error=0.99872\n",
      "Adam: Epoch 120 Error=0.99879\n",
      "Adam: Epoch 140 Error=0.99888\n",
      "Adam: Epoch 160 Error=0.99894\n",
      "Adam: Epoch 180 Error=0.99899\n",
      "Adam: Epoch 200 Error=0.99902\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99792\n",
      "Adam: Epoch 40 Error=0.99818\n",
      "Adam: Epoch 60 Error=0.99843\n",
      "Adam: Epoch 80 Error=0.99855\n",
      "Adam: Epoch 100 Error=0.99865\n",
      "Adam: Epoch 120 Error=0.99875\n",
      "Adam: Epoch 140 Error=0.99884\n",
      "Adam: Epoch 160 Error=0.99890\n",
      "Adam: Epoch 180 Error=0.99894\n",
      "Adam: Epoch 200 Error=0.99898\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.98359\n",
      "Adam: Epoch 40 Error=0.99753\n",
      "Adam: Epoch 60 Error=0.99815\n",
      "Adam: Epoch 80 Error=0.99838\n",
      "Adam: Epoch 100 Error=0.99848\n",
      "Adam: Epoch 120 Error=0.99859\n",
      "Adam: Epoch 140 Error=0.99866\n",
      "Adam: Epoch 160 Error=0.99870\n",
      "Adam: Epoch 180 Error=0.99874\n",
      "Adam: Epoch 200 Error=0.99877\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99801\n",
      "Adam: Epoch 40 Error=0.99849\n",
      "Adam: Epoch 60 Error=0.99873\n",
      "Adam: Epoch 80 Error=0.99882\n",
      "Adam: Epoch 100 Error=0.99889\n",
      "Adam: Epoch 120 Error=0.99893\n",
      "Adam: Epoch 140 Error=0.99897\n",
      "Adam: Epoch 160 Error=0.99901\n",
      "Adam: Epoch 180 Error=0.99905\n",
      "Adam: Epoch 200 Error=0.99908\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99799\n",
      "Adam: Epoch 40 Error=0.99840\n",
      "Adam: Epoch 60 Error=0.99854\n",
      "Adam: Epoch 80 Error=0.99862\n",
      "Adam: Epoch 100 Error=0.99868\n",
      "Adam: Epoch 120 Error=0.99874\n",
      "Adam: Epoch 140 Error=0.99880\n",
      "Adam: Epoch 160 Error=0.99886\n",
      "Adam: Epoch 180 Error=0.99891\n",
      "Adam: Epoch 200 Error=0.99895\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99773\n",
      "Adam: Epoch 40 Error=0.99810\n",
      "Adam: Epoch 60 Error=0.99836\n",
      "Adam: Epoch 80 Error=0.99851\n",
      "Adam: Epoch 100 Error=0.99861\n",
      "Adam: Epoch 120 Error=0.99871\n",
      "Adam: Epoch 140 Error=0.99880\n",
      "Adam: Epoch 160 Error=0.99886\n",
      "Adam: Epoch 180 Error=0.99891\n",
      "Adam: Epoch 200 Error=0.99895\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.97253\n",
      "Adam: Epoch 40 Error=0.99724\n",
      "Adam: Epoch 60 Error=0.99771\n",
      "Adam: Epoch 80 Error=0.99824\n",
      "Adam: Epoch 100 Error=0.99844\n",
      "Adam: Epoch 120 Error=0.99857\n",
      "Adam: Epoch 140 Error=0.99864\n",
      "Adam: Epoch 160 Error=0.99870\n",
      "Adam: Epoch 180 Error=0.99874\n",
      "Adam: Epoch 200 Error=0.99878\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99513\n",
      "Adam: Epoch 40 Error=0.99829\n",
      "Adam: Epoch 60 Error=0.99850\n",
      "Adam: Epoch 80 Error=0.99858\n",
      "Adam: Epoch 100 Error=0.99865\n",
      "Adam: Epoch 120 Error=0.99870\n",
      "Adam: Epoch 140 Error=0.99877\n",
      "Adam: Epoch 160 Error=0.99883\n",
      "Adam: Epoch 180 Error=0.99887\n",
      "Adam: Epoch 200 Error=0.99891\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99625\n",
      "Adam: Epoch 40 Error=0.99828\n",
      "Adam: Epoch 60 Error=0.99847\n",
      "Adam: Epoch 80 Error=0.99854\n",
      "Adam: Epoch 100 Error=0.99859\n",
      "Adam: Epoch 120 Error=0.99863\n",
      "Adam: Epoch 140 Error=0.99866\n",
      "Adam: Epoch 160 Error=0.99870\n",
      "Adam: Epoch 180 Error=0.99875\n",
      "Adam: Epoch 200 Error=0.99879\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99595\n",
      "Adam: Epoch 40 Error=0.99831\n",
      "Adam: Epoch 60 Error=0.99852\n",
      "Adam: Epoch 80 Error=0.99869\n",
      "Adam: Epoch 100 Error=0.99876\n",
      "Adam: Epoch 120 Error=0.99884\n",
      "Adam: Epoch 140 Error=0.99891\n",
      "Adam: Epoch 160 Error=0.99896\n",
      "Adam: Epoch 180 Error=0.99900\n",
      "Adam: Epoch 200 Error=0.99904\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99686\n",
      "Adam: Epoch 40 Error=0.99852\n",
      "Adam: Epoch 60 Error=0.99871\n",
      "Adam: Epoch 80 Error=0.99877\n",
      "Adam: Epoch 100 Error=0.99882\n",
      "Adam: Epoch 120 Error=0.99885\n",
      "Adam: Epoch 140 Error=0.99888\n",
      "Adam: Epoch 160 Error=0.99892\n",
      "Adam: Epoch 180 Error=0.99895\n",
      "Adam: Epoch 200 Error=0.99897\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99796\n",
      "Adam: Epoch 40 Error=0.99843\n",
      "Adam: Epoch 60 Error=0.99857\n",
      "Adam: Epoch 80 Error=0.99869\n",
      "Adam: Epoch 100 Error=0.99875\n",
      "Adam: Epoch 120 Error=0.99879\n",
      "Adam: Epoch 140 Error=0.99884\n",
      "Adam: Epoch 160 Error=0.99888\n",
      "Adam: Epoch 180 Error=0.99892\n",
      "Adam: Epoch 200 Error=0.99897\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99575\n",
      "Adam: Epoch 40 Error=0.99826\n",
      "Adam: Epoch 60 Error=0.99864\n",
      "Adam: Epoch 80 Error=0.99871\n",
      "Adam: Epoch 100 Error=0.99881\n",
      "Adam: Epoch 120 Error=0.99887\n",
      "Adam: Epoch 140 Error=0.99892\n",
      "Adam: Epoch 160 Error=0.99898\n",
      "Adam: Epoch 180 Error=0.99904\n",
      "Adam: Epoch 200 Error=0.99909\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99619\n",
      "Adam: Epoch 40 Error=0.99828\n",
      "Adam: Epoch 60 Error=0.99844\n",
      "Adam: Epoch 80 Error=0.99856\n",
      "Adam: Epoch 100 Error=0.99866\n",
      "Adam: Epoch 120 Error=0.99874\n",
      "Adam: Epoch 140 Error=0.99886\n",
      "Adam: Epoch 160 Error=0.99893\n",
      "Adam: Epoch 180 Error=0.99899\n",
      "Adam: Epoch 200 Error=0.99903\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99778\n",
      "Adam: Epoch 40 Error=0.99835\n",
      "Adam: Epoch 60 Error=0.99855\n",
      "Adam: Epoch 80 Error=0.99865\n",
      "Adam: Epoch 100 Error=0.99871\n",
      "Adam: Epoch 120 Error=0.99878\n",
      "Adam: Epoch 140 Error=0.99884\n",
      "Adam: Epoch 160 Error=0.99889\n",
      "Adam: Epoch 180 Error=0.99893\n",
      "Adam: Epoch 200 Error=0.99897\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99388\n",
      "Adam: Epoch 40 Error=0.99812\n",
      "Adam: Epoch 60 Error=0.99844\n",
      "Adam: Epoch 80 Error=0.99855\n",
      "Adam: Epoch 100 Error=0.99863\n",
      "Adam: Epoch 120 Error=0.99869\n",
      "Adam: Epoch 140 Error=0.99873\n",
      "Adam: Epoch 160 Error=0.99878\n",
      "Adam: Epoch 180 Error=0.99883\n",
      "Adam: Epoch 200 Error=0.99887\n",
      "Hidden Layers:  [30, 30, 30]\n",
      "Adam: Epoch 20 Error=0.99815\n",
      "Adam: Epoch 40 Error=0.99849\n",
      "Adam: Epoch 60 Error=0.99864\n",
      "Adam: Epoch 80 Error=0.99870\n",
      "Adam: Epoch 100 Error=0.99876\n",
      "Adam: Epoch 120 Error=0.99882\n",
      "Adam: Epoch 140 Error=0.99887\n",
      "Adam: Epoch 160 Error=0.99891\n",
      "Adam: Epoch 180 Error=0.99897\n",
      "Adam: Epoch 200 Error=0.99903\n",
      "  Hidden Layers      Train   Validate       Test      Time\n",
      "0          [10]  99.937209  99.937063  99.937063  0.013166\n",
      "1      [50, 50]  99.943558  99.939520  99.939520  0.064144\n",
      "2  [30, 30, 30]  99.951751  99.942154  99.942359  0.071395\n",
      "Took 3.01 hours\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hidden Layers</th>\n",
       "      <th>Train</th>\n",
       "      <th>Validate</th>\n",
       "      <th>Test</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[10]</td>\n",
       "      <td>99.937209</td>\n",
       "      <td>99.937063</td>\n",
       "      <td>99.937063</td>\n",
       "      <td>0.013166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[50, 50]</td>\n",
       "      <td>99.943558</td>\n",
       "      <td>99.939520</td>\n",
       "      <td>99.939520</td>\n",
       "      <td>0.064144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[30, 30, 30]</td>\n",
       "      <td>99.951751</td>\n",
       "      <td>99.942154</td>\n",
       "      <td>99.942359</td>\n",
       "      <td>0.071395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Hidden Layers      Train   Validate       Test      Time\n",
       "0          [10]  99.937209  99.937063  99.937063  0.013166\n",
       "1      [50, 50]  99.943558  99.939520  99.939520  0.064144\n",
       "2  [30, 30, 30]  99.951751  99.942154  99.942359  0.071395"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "results = run_k_fold_cross_validation(X, T, 5,\n",
    "                                      [[10], [50, 50], [30, 30, 30]],\n",
    "                                      200, 0.01, 'tanh', True)\n",
    "\n",
    "elapsed = (time.time() - start) / 60/ 60\n",
    "print(f'Took {elapsed:.2f} hours')\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79542917",
   "metadata": {},
   "source": [
    "For the first configuration with a single hidden layer of 10 neurons, the model achieved high accuracy across all three datasets. It obtained an accuracy of 99.94% on the training set, as well as on the validation and test sets. The training time for this model was relatively low at 0.013 seconds.\n",
    "\n",
    "Moving to the second configuration with two hidden layers, each consisting of 50 neurons, the model demonstrated slightly improved performance. It achieved an accuracy of 99.94% on the training set and 99.94% on the validation and test sets. However, the training time increased to 0.064 seconds, indicating a longer computational effort.\n",
    "\n",
    "The third configuration, comprising three hidden layers with 30 neurons in each layer, showed the best performance among the analyzed models. This model achieved an accuracy of 99.95% on the training set, 99.94% on the validation set, and 99.94% on the test set. The training time increased further to 0.071 seconds.\n",
    "\n",
    "If we consider the best model to be the one with the highest perecntage correct for the validation set. The third model is the best fit model.\n",
    "\n",
    "Initially, we were skeptical of the high percent correct values considering the imbalanced dataset, but the values for train, validation and test models appear to be around the same range showing a good non-overfitting model.\n",
    "\n",
    "In summary, the results suggest that increasing the number of hidden layers and neurons generally improves the model's accuracy in classifying credit card fraud. However, it is important to consider the trade-off between accuracy and training time. While more complex configurations achieve higher accuracy, they also require longer training times. Therefore, it is crucial to strike a balance based on the specific requirements of the application.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f8f0386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "140ab639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Epoch 20 Error=0.99720\n",
      "Adam: Epoch 40 Error=0.99815\n",
      "Adam: Epoch 60 Error=0.99838\n",
      "Adam: Epoch 80 Error=0.99848\n",
      "Adam: Epoch 100 Error=0.99855\n",
      "Adam: Epoch 120 Error=0.99861\n",
      "Adam: Epoch 140 Error=0.99866\n",
      "Adam: Epoch 160 Error=0.99872\n",
      "Adam: Epoch 180 Error=0.99877\n",
      "Adam: Epoch 200 Error=0.99883\n"
     ]
    }
   ],
   "source": [
    "nnet =  NeuralNetworkClassifier(X_train.shape[1], [30,30,30], len(classes))\n",
    "nnet.train(X_train, T_train, 200, 0.01, method='adam', verbose=True)\n",
    "Y_tclasses, Y_tprobs = nnet.use(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "164b92b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC\n",
      "0.9467938697024044\n",
      "F1-Score\n",
      "0.8428571428571429\n",
      "Accuracy\n",
      "0.9994850368081645\n"
     ]
    }
   ],
   "source": [
    "print('AUC-ROC')\n",
    "print(roc_auc_score(Y_tclasses, T_test))\n",
    "      \n",
    "print('F1-Score')\n",
    "print(f1_score(Y_tclasses, T_test))\n",
    "    \n",
    "print('Accuracy')\n",
    "print(accuracy_score(Y_tclasses, T_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503e2b6a",
   "metadata": {},
   "source": [
    "AUC-ROC (Area Under the Receiver Operating Characteristic Curve) is a metric that measures the ability of the model to distinguish between positive and negative classes. The AUC-ROC ranges from 0 to 1, with a higher value indicating a better performance of the model. From the above table, the AUC-ROC value is 0.9468 which indicates that the model has good predictive power and is able to effectively distinguish between positive and negative classes.\n",
    "\n",
    "F1-Score is a metric that combines precision and recall, two key metrics in classification. It ranges from 0 to 1, with a higher value indicating a better performance of the model. Precision measures the proportion of true positives among all positive predictions, while recall measures the proportion of true positives among all actual positive cases. A high precision score indicates that the model is making fewer false-positive errors (i.e., the model is more selective in identifying fraudulent transactions), while a high recall score indicates that the model is making fewer false-negative errors (i.e., the model is more sensitive in detecting fraudulent transactions). In the case of credit card fraud detection, we want to maximize both precision and recall to minimize the number of false positives and false negatives. The F1-score, which is the harmonic mean of precision and recall, provides a single score that balances the trade-off between precision and recall. From above results, the F1-Score value of 0.8429 indicates that the model has a good balance between precision and recall. \n",
    "\n",
    "Accuracy is a metric that measures the overall correctness of the model predictions. It ranges from 0 to 1, with a higher value indicating a better performance of the model. The Accuracy value of 0.9995 that is obtained above indicates that the model has a high degree of accuracy in predicting both positive and negative classes.\n",
    "\n",
    "Overall, the combination of these metrics suggests that the model has a high degree of predictive power and is able to effectively distinguish between positive and negative classes. However, it is important to note that these metrics should be used in conjunction with other evaluation techniques and domain knowledge to assess the overall performanceÂ ofÂ theÂ model.\n",
    "\n",
    "Therefore, it is important to evaluate the model's performance using multiple metrics and not just rely on the accuracy score alone. Let's analyze confusion matrix to get a better idea of how much of a good fit the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ac949ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y_classes, T):\n",
    "    class_names = np.unique(T)\n",
    "    table = []\n",
    "    for true_class in class_names:\n",
    "        row = []\n",
    "        for Y_class in class_names:\n",
    "            row.append(100 * np.mean(Y_classes[T == true_class] == Y_class))\n",
    "        table.append(row)\n",
    "    conf_matrix = pd.DataFrame(table, index=class_names, columns=class_names)\n",
    "    return conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8bba14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy in percent correct: 99.95\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row0_col0,#T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row1_col1{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row0_col1,#T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row1_col0{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row0_col0\" class=\"data row0 col0\" >100.0%</td>\n",
       "                        <td id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row0_col1\" class=\"data row0 col1\" >0.0%</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row1_col0\" class=\"data row1 col0\" >20.3%</td>\n",
       "                        <td id=\"T_ddb61c80_f030_11ed_b76a_dc4a3e7f1158row1_col1\" class=\"data row1 col1\" >79.7%</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ff2c99b0780>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perc_correct = 100 * np.mean(Y_tclasses == T_test)\n",
    "print(f'Test accuracy in percent correct: {perc_correct:.2f}')\n",
    "cm = confusion_matrix(Y_tclasses, T_test)\n",
    "cm.style.background_gradient(cmap='Blues').format(\"{:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f4086",
   "metadata": {},
   "source": [
    "The above confusion matrix shows that out of all non-fraudulent transactions, the model correctly predicted 100% of them as non-fraudulent (true negatives). However, out of all fraudulent transactions, the model only correctly predicted 79.7% of them as fraudulent (true positives). This means that the model incorrectly predicted 20.3% of fraudulent transactions as non-fraudulent (false negatives). Overall, it indicates that the model is not as efficient for fraudulent transactions as it is for non-fraudulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b229a6",
   "metadata": {},
   "source": [
    "## Sklearn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816ee4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e0d6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop(labels='Class', axis=1) # Features\n",
    "T = df1.loc[:,'Class']               # Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2870875f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.62      0.56      0.59        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.81      0.78      0.79     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n",
      "AUC-ROC:  0.7803132859784319\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "Xtrain, Xtest, Ttrain, Ttest = train_test_split(X, T, test_size=0.2, random_state=42)\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(Xtrain, Ttrain)\n",
    "Y_pred = classifier.predict(Xtest)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(Ttest, Y_pred))\n",
    "auc_roc = roc_auc_score(Ttest, Y_pred)\n",
    "print(\"AUC-ROC: \", auc_roc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9743284a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred1 = Y_pred.reshape(-1,1)\n",
    "Y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "213c61f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy in percent correct: 99.86\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row0_col0,#T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row1_col1{\n",
       "            background-color:  #08306b;\n",
       "            color:  #f1f1f1;\n",
       "        }#T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row0_col1,#T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row1_col0{\n",
       "            background-color:  #f7fbff;\n",
       "            color:  #000000;\n",
       "        }</style><table id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row0_col0\" class=\"data row0 col0\" >99.9 %</td>\n",
       "                        <td id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row0_col1\" class=\"data row0 col1\" >0.1 %</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row1_col0\" class=\"data row1 col0\" >43.9 %</td>\n",
       "                        <td id=\"T_8dc95ae8_f076_11ed_939c_dc4a3e7f1158row1_col1\" class=\"data row1 col1\" >56.1 %</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7d34261e48>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "perc_correct = 100 * np.mean(Y_pred == Ttest)\n",
    "print(f'Test accuracy in percent correct: {perc_correct:.2f}')\n",
    "cm = confusion_matrix(Y_pred1, Ttest)\n",
    "cm.style.background_gradient(cmap = 'Blues').format(\"{:.1f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05fc8ea",
   "metadata": {},
   "source": [
    "From the confusion matrix we can say that the model accurately identified 99.9% of the non-fraudulent transactions (True Negatives, TN). This high percentage indicates that the model has a strong ability to correctly classify non-fraudulent transactions.\n",
    "\n",
    "There were a small number of non-fraudulent transactions (0.1%) that the model incorrectly predicted as fraudulent (False Positives, FP). Although this percentage is low, it still represents a small fraction of misclassified transactions.\n",
    "\n",
    "On the other hand, the model struggled to accurately identify fraudulent transactions (class 1). It incorrectly classified 43.9% of the fraudulent transactions as non-fraudulent (False Negatives, FN). This high percentage suggests that a significant number of fraudulent transactions were either missed or misclassified.\n",
    "\n",
    "The model did manage to correctly predict 56.1% of the fraudulent transactions (True Positives, TP), indicating that it captured a portion of the fraudulent activity.\n",
    "\n",
    "In summary, the logistic regression model demonstrates good performance in accurately predicting non-fraudulent transactions. However, it has limitations in accurately identifying fraudulent transactions, with a relatively high rate of false negatives. This suggests that further improvements or adjustments may be necessary to enhance the model's ability to detect fraudulent activity effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442273a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
